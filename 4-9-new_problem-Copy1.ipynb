{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from config import *\n",
    "# context = Context('amended-1-no_noise', debug=0, dep_var=dep_var2, mulr=0, load_data=False)\n",
    "context = Context('amended-2-14', debug=0, dep_var=dep_var2, mulr=14, load_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list existing data files\n",
    "print('Existing data files:', [o.stem for o in Path(context.fn_feather).parent.glob('*.feather')])\n",
    "\n",
    "fmtr = DataFormatter(context, cycle_feathers=context.fn_cycles)\n",
    "cycles = fmtr.cycles # shorten notation\n",
    "# fmtr.drop_redundent_columns() # already done for cycles\n",
    "idx = fmtr.beginning_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target data format: `[flattened matrix: 30x150, extra cont: 1x2, extra cat: n]`\n",
    "\n",
    "parameters: cat_names\n",
    "\n",
    "Use no cat for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conts, stat_x = fmtr.get_x(normalize=True)\n",
    "extra_x = fmtr.get_extra_x_task2()\n",
    "deps = fmtr.get_y()\n",
    "stat_y = [o.loc[context.dep_var] for o in stat_x]\n",
    "stat_extra_x = [o.loc[dep_var1] for o in stat_x]\n",
    "train_cont = flatten_and_cat(df_conts, [extra_x, deps], sl=context.sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cont.to_feather(context.fn_feather)\n",
    "np.savez_compressed(context.fn_np, idx=idx, stat_x=stat_x, stat_y=stat_y, stat_extra_x=stat_extra_x)\n",
    "context.fn_feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = Context('amended-1-no_noise', debug=0, dep_var=dep_var2, mulr=1)\n",
    "context = Context('amended-2-14', debug=0, dep_var=dep_var2, mulr=14)\n",
    "databunch = MultiDeptTabularDataBunch.from_df('tmp', context.cyc_cont, context.dep_var, valid_idx=context.valid_idx_tile, bs=context.bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ni(databunch.train_dl)\n",
    "[o.shape for o in x], y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_extra_x = False\n",
    "hyper_params = {\n",
    "    'n_hidden': 400,\n",
    "    'n_layers': 3,\n",
    "    'hidden_p': 0.3,\n",
    "    'input_p': 0.4,\n",
    "    'weight_p': 0.5,\n",
    "    'layers': [3*(context.n_cont + int(use_extra_x) * 2), 5*context.n_cont, 5, len(context.dep_var)],\n",
    "    'drops': [0.4] * 4,\n",
    "    'alpha': 2.,\n",
    "    'beta': 1.,\n",
    "    'clip': 0.5,\n",
    "    'loss_func': MSELossFlat()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingLinearClassifier(nn.Module):                                                                                                                                                    \n",
    "    \"Create a linear classifier with pooling.\"                                                                                                                                               \n",
    "                                                                                                                                                                                             \n",
    "    def __init__(self, layers:Collection[int], drops:Collection[float]):                                                                                                                     \n",
    "        super().__init__()                                                                                                                                                                   \n",
    "        mod_layers = []                                                                                                                                                                      \n",
    "        activs = [nn.ReLU(inplace=True)] * (len(layers) - 2) + [None]                                                                                                                        \n",
    "        for n_in,n_out,p,actn in zip(layers[:-1],layers[1:], drops, activs):                                                                                                                 \n",
    "            mod_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn)                                                                                                                           \n",
    "        self.layers = nn.Sequential(*mod_layers)                                                                                                                                             \n",
    "                                                                                                                                                                                             \n",
    "    def pool(self, x:Tensor, bs:int, is_max:bool):                                                                                                                                           \n",
    "        \"Pool the tensor along the seq_len dimension.\"                                                                                                                                       \n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d                                                                                                                       \n",
    "        return f(x.transpose(1,2), (1,)).view(bs,-1)                                                                                                                                         \n",
    "                                                                                                                                                                                             \n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:                                                                                                              \n",
    "        raw_outputs, outputs = input                                                                                                                                                         \n",
    "        output = outputs[-1]                                                                                                                                                                 \n",
    "        bs,sl,_ = output.size()                                                                                                                                                              \n",
    "        avgpool = self.pool(output, bs, False)                                                                                                                                               \n",
    "        mxpool = self.pool(output, bs, True)                                                                                                                                                 \n",
    "        x = torch.cat([output[:,-1], mxpool, avgpool], 1)                                                                                                                                    \n",
    "        x = self.layers(x)                                                                                                                                                                   \n",
    "        return x, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task2Model(AWD_LSTM):\n",
    "    def __init__(self, n_cont, emb_sz:int, n_hid:int, n_layers:int, layers:Collection[int], drops:Collection[float], hidden_p:float=0.2,\n",
    "                 input_p:float=0.6, embed_p:float=0.1, weight_p:float=0.5, bidir:bool=False, \n",
    "                 sl=30):\n",
    "        # note: check pad_token when generating cat variables\n",
    "        qrnn = False # continuous variables only for this model\n",
    "        vocab_sz,pad_token=1,0 # temp\n",
    "        super().__init__(vocab_sz, n_cont, n_hid, n_layers, pad_token, hidden_p, input_p, embed_p, weight_p, qrnn, bidir)\n",
    "        self.sl,self.n_cont,self.emb_sz = sl,n_cont,emb_sz\n",
    "        self.pool_linear_classifier = PoolingLinearClassifier(layers, drops)\n",
    "        \n",
    "        # re-initialize embedding layer\n",
    "        vocab_sz,pad_token=1,0\n",
    "        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n",
    "        \n",
    "    def forward(self, x_cat:Tensor, x_cont:Tensor):\n",
    "        bs,*_ = x_cat.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        x_cont, x_extra = x_cont[:,:self.n_cont*self.sl], x_cont[:,self.n_cont*self.sl:]\n",
    "        x_cont = x_cont.view(bs, self.sl, self.n_cont)\n",
    "        x_cat = self.encoder_dp(x_cat)\n",
    "\n",
    "        raw_output = self.input_dp(x_cont)\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        self.hidden = to_detach(new_hidden, cpu=False)\n",
    "        \n",
    "        # fc\n",
    "        x, raw_outputs, outputs = self.pool_linear_classifier((raw_outputs, outputs))\n",
    "        return x, raw_outputs, outputs\n",
    "    \n",
    "    def _one_hidden(self, l:int)->Tensor:\n",
    "        \"Return one hidden state.\"\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.n_cont) // self.n_dir\n",
    "        return one_param(self).new(1, self.bs, nh).zero_()\n",
    "    \n",
    "emb_sz = 3 # use rule of thumb later\n",
    "model = Task2Model(context.n_cont,emb_sz, hyper_params['n_hidden'], hyper_params['n_layers'], hyper_params['layers'], hyper_params['drops'], \n",
    "                  hidden_p=hyper_params['hidden_p'], input_p=hyper_params['input_p'], weight_p=hyper_params['weight_p'])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, raw_outputs, outputs = model(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNNTrainer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = get_new_model(context, databunch, hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ni(learner.data.train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.metrics[0].stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(10, lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(20, lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(20, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(20, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(20, lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(40, lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.cyc_cont.iloc[:,-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('new_model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load(context.fn_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz['stat_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra = denormalize(context.cyc_cont.iloc[:,-4:-2], *npz['stat_extra_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra.mean(), extra.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
