{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 桩号预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from config import *\n",
    "context = Context('validation', gpu_start=4, dep_var=['桩号'], debug=0, mulr=1, fn_feather='tbmData/feather/removed_redundent1-full-30-no_noise-zhuanghao.feather')\n",
    "databunch = MultiDeptTabularDataBunch.from_df('tmp', context.cyc_cont, context.dep_var, valid_idx=context.valid_idx_tile, bs=context.bs)\n",
    "\n",
    "preload_pytorch()\n",
    "record = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'n_hidden': 100,\n",
    "    'n_layers': 3,\n",
    "    'hidden_p': 0.3,\n",
    "    'input_p': 0.6,\n",
    "    'weight_p': 0.5,\n",
    "    'layers': [3*context.n_cont, 5, len(context.dep_var)],\n",
    "    'drops': [0.2] * 3,\n",
    "    'alpha': 2.,\n",
    "    'beta': 1.,\n",
    "    'clip': 0.5,\n",
    "#     'loss_func' = MSELossFlat()\n",
    "    'loss_func': l1\n",
    "}\n",
    "\n",
    "learner = get_cont_model(context, databunch, hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('removed_redundent1-full-30-7-zhuanghao');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = learner.data.one_batch(DatasetType.Valid, detach=False, denorm=False)\n",
    "#     xb,yb = learner.data.one_batch(DatasetType.Train, detach=False, denorm=False)\n",
    "# remove noise\n",
    "# xb = [o[::context.mulr] for o in xb]\n",
    "# yb = yb[::context.mulr]\n",
    "\n",
    "yb = to_detach(yb)\n",
    "cb_handler = CallbackHandler(learner.callbacks) # rnn trainer\n",
    "learner.model.reset()\n",
    "pb, _ = loss_batch(learner.model.eval(), xb, yb, cb_handler=cb_handler)\n",
    "loss, mapd = learner.loss_func(pb, yb), learner.metrics[0](pb, yb)\n",
    "if 1:\n",
    "    print('Predicting mean: mapd =', learner.metrics[0](pb, pb.mean(0)))\n",
    "print('mapd', to_np(mapd))\n",
    "y_np, p_np = (to_np(o).squeeze() for o in (yb,pb))\n",
    "\n",
    "# Plot p against y. Expect linearity\n",
    "yb_denormed, pb_denormed = denormalize(y_np, *context.stat_y), denormalize(p_np, *context.stat_y)\n",
    "df = pd.DataFrame({'y': yb_denormed, 'p': pb_denormed})\n",
    "sns.jointplot(x='y', y='p', data=df, kind=\"reg\")\n",
    "\n",
    "# reshape x\n",
    "# xb_r = to_np(xb[1]).reshape(-1,len(context.cont_names),context.sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'y': yb_denormed, 'p': pb_denormed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.stat_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crits import MAPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapd = MAPD(stats=context.stat_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapd(yb, yb.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapd(yb, pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original data\n",
    "fmtr = DataFormatter(context, cycle_feathers=context.fn_cycles)\n",
    "cycles = fmtr.cycles # shorten notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb_or = context.cyc_cont.values[:,:-2].reshape(context.cyc_cont.shape[0],len(context.cont_names),context.sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.data.valid_ds[i][0].data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = np.array([learner.data.valid_ds[i][0].data[1].numpy() for i in range(5)])\n",
    "# reshape x\n",
    "xb_r = xb.reshape(-1,len(context.cont_names),context.sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 500\n",
    "(xb_r[:i] - xb_or[:i]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem here. The validation samples is not the first few samples of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(learner):\n",
    "    xb,yb = learner.data.one_batch(DatasetType.Valid, detach=False, denorm=False)\n",
    "#     xb,yb = learner.data.one_batch(DatasetType.Train, detach=False, denorm=False)\n",
    "    \n",
    "    yb = to_detach(yb)\n",
    "    cb_handler = CallbackHandler(learner.callbacks) # rnn trainer\n",
    "    learner.model.reset()\n",
    "    pb, _ = loss_batch(learner.model.eval(), xb, yb, cb_handler=cb_handler)\n",
    "    loss, mapd = learner.loss_func(pb, yb), learner.metrics[0](pb, yb)\n",
    "    y_np, p_np = (to_np(o) for o in (yb,pb))\n",
    "    \n",
    "    # Plot p against y. Expect linearity\n",
    "    yb_denormed, pb_denormed = denormalize(y_np, *context.stat_y), denormalize(p_np, *context.stat_y)\n",
    "    i=0;sns.jointplot(x=yb_denormed[:,i], y=pb_denormed[:,i], kind=\"reg\")\n",
    "    i=1;sns.jointplot(x=yb_denormed[:,i], y=pb_denormed[:,i], kind=\"reg\")\n",
    "    \n",
    "    return loss, mapd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mapd = validate(learner); mapd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare X of different noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(x, context):\n",
    "    if isinstance(x, Tensor): x = to_np(x)\n",
    "    x = x.reshape(context.sl, len(context.cont_names))\n",
    "    return pd.DataFrame(x, columns=context.cont_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without noise\n",
    "ax = None\n",
    "for i in range(3):\n",
    "    df = reconstruct(xb_np[i], context)\n",
    "    ax = plots(df, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
